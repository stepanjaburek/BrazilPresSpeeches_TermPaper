# -*- coding: utf-8 -*-
"""Brazil_Presidential_Speeches.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NzpaP7VLeYjWoYc3YzqhZG-d7Y22CJE3

<a href="https://colab.research.google.com/github/stepanjaburek/workingpaper_czech_psp_speeches/blob/main/streamline_translation_sentiment.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>

# **Machine translation using the Opus-MT model from Uni Helsinky**

# Setup
"""

!pip install transformers sentencepiece sacremoses torch tqdm

import pandas as pd
from transformers import MarianMTModel, MarianTokenizer
from tqdm.notebook import tqdm
import torch
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

"""# **Machine translation using the NLLB-200 model from Meta**"""

def translate_csv_nllb(file_path, source_lang='por_Latn', target_lang='eng_Latn', batch_size=16, max_length=256):

    df = pd.read_csv(file_path)

    model_name = 'facebook/nllb-200-distilled-600M'
    tokenizer = AutoTokenizer.from_pretrained(model_name, src_lang=source_lang)


    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    dtype = torch.float16 if device == 'cuda' else torch.float32

    model = AutoModelForSeq2SeqLM.from_pretrained(
        model_name,
        torch_dtype=dtype,
        device_map=device
    )
    target_token_id = tokenizer.convert_tokens_to_ids(target_lang)

    if device == 'cuda':
        torch.cuda.empty_cache()

    # Process in batches more efficiently
    translated_texts = []

    # Use a larger batch size on powerful hardware
    effective_batch_size = batch_size
    if device == 'cuda':
        try:
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # in GB
            if gpu_memory > 20:
                effective_batch_size = min(32, batch_size)
        except:
            # In case we can't get GPU properties
            pass

    # Use tqdm for progress tracking
    for i in tqdm(range(0, len(df), effective_batch_size), desc="Translating batches"):
        batch_texts = df['context_full'][i:i + effective_batch_size].tolist()

        # Skip empty texts
        batch_texts = [text for text in batch_texts if isinstance(text, str) and text.strip()]
        if not batch_texts:
            continue

        # Tokenize
        inputs = tokenizer(
            batch_texts,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=max_length
        ).to(device)

        # Generate with optimized parameters
        with torch.no_grad():  # Disable gradient calculation for inference
            outputs = model.generate(
                **inputs,
                forced_bos_token_id=target_token_id,
                max_length=max_length,
                num_beams=4,  # Beam search for better quality
                length_penalty=1.0,
                early_stopping=True
            )

        # Decode the outputs
        batch_translations = tokenizer.batch_decode(outputs, skip_special_tokens=True)
        translated_texts.extend(batch_translations)

        # Clean up memory
        if device == 'cuda':
            del inputs, outputs
            torch.cuda.empty_cache()

    # Add translations to the dataframe
    df['translated_context_full'] = translated_texts

    return df

translated_df = translate_csv_nllb('/content/working_class.csv')
translated_df.to_csv('working_class_translated.csv', index=False)

translated_df = translate_csv_nllb('/content/middle_class.csv')
translated_df.to_csv('middle_class_translated.csv', index=False)

translated_df = translate_csv_nllb('/content/elite_class.csv')
translated_df.to_csv('elite_class_translated.csv', index=False)

"""# **Sentiment classification using the Political DEBATE model by Burnham et al. (2024)**

# Setup
"""

import transformers
from transformers import pipeline
from datasets import Dataset

"""# Model specification and labeling - Working class"""

def analyze_sentiments(df, classifier, classes, hypothesis, batch_size=16):
    results = []
    for i in tqdm(range(0, len(df), batch_size)):
        batch_output = classifier(
            df['translated_context_full'][i:i + batch_size].tolist(),
            classes,
            hypothesis_template=hypothesis,
            multi_label=False,
            batch_size=batch_size
        )

        for item in batch_output:
            results.append({
                'label': item['labels'][0],
                'score': item['scores'][0],
                **{f'{label}_score': score for label, score in zip(item['labels'], item['scores'])}
            })

    return pd.DataFrame(results)

model_name = "mlburnham/Political_DEBATE_large_v1.0"
hypothesis_template =  "The emotional valence of this text towards the working class is {}"
classes = [ "negative", "neutral", "positive"]

device = 0 if torch.cuda.is_available() else -1
classifier = pipeline("zero-shot-classification",
                     model=model_name,
                     device=device)

df = pd.read_csv("/content/working_class_translated.csv")
results = analyze_sentiments(df, classifier, classes, hypothesis_template)

pd.concat([df, results], axis=1).to_csv('debate_sentiment_working_class.csv', index=False)


print("\nSentiment Distribution:")
print(results['label'].value_counts())

"""# Model specification and labeling - Middle class"""

model_name = "mlburnham/Political_DEBATE_large_v1.0"
hypothesis_template =  "The emotional valence of this text towards the middle class is {}"
classes = [ "negative", "neutral", "positive"]

device = 0 if torch.cuda.is_available() else -1
classifier = pipeline("zero-shot-classification",
                     model=model_name,
                     device=device)

df = pd.read_csv("/content/middle_class_translated.csv")
results = analyze_sentiments(df, classifier, classes, hypothesis_template)


pd.concat([df, results], axis=1).to_csv('debate_sentiment_middle_class.csv', index=False)


print("\nSentiment Distribution:")
print(results['label'].value_counts())

"""# Model specification and labeling - Elite:"""

model_name = "mlburnham/Political_DEBATE_large_v1.0"
hypothesis_template =  "The emotional valence of this text towards the elites is {}"
classes = [ "negative", "neutral", "positive"]

device = 0 if torch.cuda.is_available() else -1
classifier = pipeline("zero-shot-classification",
                     model=model_name,
                     device=device)

df = pd.read_csv("/content/elite_class_translated.csv")
results = analyze_sentiments(df, classifier, classes, hypothesis_template)


pd.concat([df, results], axis=1).to_csv('debate_sentiment_elite_class.csv', index=False)


print("\nSentiment Distribution:")
print(results['label'].value_counts())